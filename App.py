# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bdIdHqpK-2yu8lLUDgxoZn4p_Rkf5vuW
"""

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
import joblib
import requests
from io import BytesIO

# Function to download model or vectorizer from GitHub
def load_model(url):
    response = requests.get(url)
    response.raise_for_status()  # Check that the request was successful
    return joblib.load(BytesIO(response.content))

# Load your model and vectorizer
model_url = 'https://github.com/Divya-coder-isb/F-B/blob/main/best_xgboost_model.joblib?raw=true'
vectorizer_url = 'https://github.com/Divya-coder-isb/F-B/blob/main/tfidf_vectorizer.joblib?raw=true'
model = load_model(model_url)
vectorizer = load_model(vectorizer_url)

# Streamlit User Interface
st.title('Model Fairness and Bias Evaluation Dashboard')

# Sidebar for user inputs
threshold = st.sidebar.slider('Classification Threshold', 0.0, 1.0, 0.237, 0.01)

# Sample input for text classification
user_input = st.text_area("Enter text for toxicity prediction:")

# Functions to calculate fairness metrics
def calculate_demographic_parity(predictions, sensitive_attrs):
    rates = predictions.groupby(sensitive_attrs).mean()
    return rates.to_dict()

def calculate_predictive_parity(predictions, labels, sensitive_attrs):
    df = pd.DataFrame({'predictions': predictions, 'labels': labels, 'group': sensitive_attrs})
    positive_pred = df[df['labels'] == 1]
    rates = positive_pred.groupby('group').mean()
    return rates['predictions'].to_dict()

def plot_roc_curve(y_true, y_scores):
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.grid(True)
    st.pyplot(plt)

# Preprocess and predict
if st.button('Predict'):
    transformed_input = vectorizer.transform([user_input])
    proba = model.predict_proba(transformed_input)[0, 1]
    prediction = (proba >= threshold).astype(int)
    st.write('Probability of Toxicity:', proba)
    st.write('Toxic' if prediction else 'Not Toxic')

    # Simulate sensitive attributes and labels
    sensitive_attrs = np.random.choice(['male', 'female', 'non-binary'], size=100)
    labels = np.random.randint(0, 2, size=100)
    predictions = np.random.rand(100) > 0.5

    # Calculate and display fairness metrics
    dp = calculate_demographic_parity(pd.Series(predictions), pd.Series(sensitive_attrs))
    pp = calculate_predictive_parity(pd.Series(predictions), pd.Series(labels), pd.Series(sensitive_attrs))
    st.write("## Fairness Metrics")
    st.write("Demographic Parity:", dp)
    st.write("Predictive Parity:", pp)

    # ROC Curve
    plot_roc_curve(labels, predictions)